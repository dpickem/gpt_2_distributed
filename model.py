#!/usr/bin/env python
"""
Pure-PyTorch GPT-2 implementation.

The initial implementation is mostly generated by OpenAI's O3-Pro model.
Additional documentation and explanations are provided by the author.

Features and design choices:

- Causal masking    : A single tril mask kept on the module buffer—efficient and GPU-friendly.
- OpenAI weight init: N(0, 0.02) on all linear / embedding layers exactly as in the original repo.
- LayerNorm → pre-LN: Matches GPT-2 revision B (stabler training with AdamW).
- Weight tying      : lm_head.weight = wte.weight, cutting ~38 M params.
"""

import math, torch
import torch.nn as nn
import torch.nn.functional as F
from dataclasses import dataclass
from typing import Optional, Tuple


# ------------------------------------------------------------------------------------------------
#                                       0.  Configuration
# ------------------------------------------------------------------------------------------------
@dataclass(frozen=True)
class GPT2Config:
    """
    Configuration for the GPT-2 model.
    """

    # The total vocabulary size (i.e. supported number of tokens)
    vocab_size: int = 50257

    # The maximum sequence length (i.e. the maximum number of tokens in a single sequence)
    n_positions: int = 1024

    # The dimension of the embeddings.
    n_embd: int = 768

    # The number of layers
    n_layer: int = 12

    # The number of attention heads
    n_head: int = 12

    # The dropout rate for the residual connections.
    resid_pdrop: float = 0.1

    # The dropout rate for the attention mechanism.
    attn_pdrop: float = 0.1

    # The epsilon for the layer normalization.
    layer_norm_eps: float = 1e-5

    # The standard deviation for the weight initialization.
    initializer_range: float = 0.02  # matches OpenAI paper


# ------------------------------------------------------------------------------------------------
#                                       1.  Building blocks
# ------------------------------------------------------------------------------------------------
class NewGELU(nn.Module):
    """Careful there are a few versions of GeLU, this one is the exact one used by OpenAI"""

    def forward(self, input):
        return (
            0.5
            * input
            * (
                1.0
                + torch.tanh(
                    math.sqrt(2.0 / math.pi)
                    * (input + 0.044715 * torch.pow(input, 3.0))
                )
            )
        )


class CausalMultiHeadSelfAttention(nn.Module):
    """Multi-head self-attention with a *causal* mask (no tokens see the future)."""

    def __init__(self, cfg: GPT2Config):
        super().__init__()

        # Verify that the number of embedding dimensions is divisible by the number of heads.
        assert cfg.n_embd % cfg.n_head == 0
        self.n_head = cfg.n_head
        self.head_dim = cfg.n_embd // cfg.n_head

        # Linear layers for the query, key, and value projections.
        # NOTE: The qkv linear layer projects the input embeddings dimension to 3 * cfg.n_embd
        #       because it stores the weights for the query, key, and value projections, which each
        #       output tensors of cfg.n_embd.
        self.qkv = nn.Linear(cfg.n_embd, 3 * cfg.n_embd)
        self.proj = nn.Linear(cfg.n_embd, cfg.n_embd)

        # Dropout layers for the attention and residual connections.
        self.attn_drop = nn.Dropout(cfg.attn_pdrop)
        self.resid_drop = nn.Dropout(cfg.resid_pdrop)

        # Pre-compute a causal mask [seq, seq] once; reuse every forward().
        # NOTE: This is a lower triangular matrix with ones on the diagonal and below.
        #       It is used to mask out the future tokens from the attention mechanism.
        mask = torch.tril(torch.ones(cfg.n_positions, cfg.n_positions)).view(
            1, 1, cfg.n_positions, cfg.n_positions
        )
        self.register_buffer("mask", mask, persistent=False)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # Extract the batch size, sequence length, and embedding dimension from the input tensor.
        # NOTE: x.size() returns the same as x.shape, but x.size() is a method.
        B, T, C = x.size()  # [batch, time, channels]

        # Project the input embeddings into query, key, and value tensors.
        qkv = self.qkv(x)  # [B, T, 3*C]

        # The following line accomplishes multiple steps:
        # 1. Reshape the tensor from [B, T, 3*C] to [B, T, 3, n_head, head_dim], essentially
        #    decomposing C into n_head * head_dim.
        # 2. Transpose the tensor from [B, T, 3, n_head, head_dim] to [B, n_head, 3, T, head_dim]
        #    This is done to move the head dimension forward for efficient parallel processing of
        #    attention heads.
        qkv = qkv.view(B, T, 3, self.n_head, self.head_dim).transpose(1, 3)

        # Unbind the tensor into query, key, and value tensors, where unbind splits the tensor along
        # the second dimension (i.e. the "3" dimension).
        # NOTE: The q, k, and v tensors are each of shape [B, n_head, T, head_dim].
        q, k, v = qkv.unbind(dim=2)  # each: [B, n_head, T, head_dim]

        # Compute the attention scores as the scaled dot product of the query and key tensors.
        # NOTE: Both q and k are of shape [B, n_head, T, head_dim].
        # NOTE: The transpose(-2, -1) swaps the last two dimensions of the tensor, i.e. changes from
        #       [B, n_head, T, head_dim] to [B, n_head, head_dim, T].
        # NOTE: The matrix multiplication operator @ computes
        #       [B, n_head, T, head_dim] @ [B, n_head, head_dim, T] -> [B, n_head, T, T]
        att = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)

        # Mask out the future tokens from the attention mechanism.
        # NOTE: The mask is a lower triangular matrix with ones on the diagonal and below.
        # NOTE: self.mask is of shape [1, 1, max_seq_len, max_seq_len].
        # NOTE: The masked_fill function sets the elements of the tensor to -1e4 where the mask is
        #       0 (i.e. masked_fill(condition, value)).
        att = att.masked_fill(self.mask[:, :, :T, :T] == 0, -1e4)
        att = F.softmax(att, dim=-1)
        att = self.attn_drop(att)

        # Compute the output of the attention mechanism as the weighted sum of the value tensors.
        # NOTE: The matrix multiplication operator @ computes
        #       [B, n_head, T, T] @ [B, n_head, T, head_dim] -> [B, n_head, T, head_dim]
        y = att @ v

        # Transpose the tensor from [B, n_head, T, head_dim] to [B, T, n_head, head_dim] and
        # flatten the last two dimensions to [B, T, C].
        y = y.transpose(1, 2).contiguous().view(B, T, C)

        # Apply the residual dropout and projection to the output.
        y = self.resid_drop(self.proj(y))
        return y


class MLP(nn.Module):
    """
    Multi-layer perceptron.
    """

    def __init__(self, cfg: GPT2Config):
        super().__init__()

        # The hidden dimension is 4 times the embedding dimension.
        hidden = 4 * cfg.n_embd

        # The first linear layer projects the input embeddings to the hidden dimension.
        self.fc1 = nn.Linear(cfg.n_embd, hidden)

        # The second linear layer projects the hidden dimension back to the embedding dimension.
        self.fc2 = nn.Linear(hidden, cfg.n_embd)

        # The activation function is NewGELU.
        self.act = NewGELU()

        # Dropout layers for the residual connections.
        self.drop1 = nn.Dropout(cfg.resid_pdrop)
        self.drop2 = nn.Dropout(cfg.resid_pdrop)

    def forward(self, x):
        # Apply the first linear layer, the activation function, and the first dropout layer.
        x = self.drop1(self.act(self.fc1(x)))

        # Apply the second linear layer and the second dropout layer.
        x = self.drop2(self.fc2(x))
        return x


class GPT2Block(nn.Module):
    """
    A single transformer block.
    """

    def __init__(self, cfg: GPT2Config):
        super().__init__()

        # Layer normalization for the input embeddings.
        self.ln1 = nn.LayerNorm(cfg.n_embd, eps=cfg.layer_norm_eps)

        # The attention mechanism.
        self.attn = CausalMultiHeadSelfAttention(cfg)

        # Layer normalization for the output of the attention mechanism.
        self.ln2 = nn.LayerNorm(cfg.n_embd, eps=cfg.layer_norm_eps)
        self.mlp = MLP(cfg)

    def forward(self, x):
        # Apply the attention mechanism and add the residual connection.
        x = x + self.attn(self.ln1(x))  # pre‑LN + residual

        # Apply the MLP and add the residual connection.
        x = x + self.mlp(self.ln2(x))
        return x


# ------------------------------------------------------------------------------------------------
#                                       2.  The GPT‑2 backbone
# ------------------------------------------------------------------------------------------------
class GPT2Backbone(nn.Module):
    """
    The GPT-2 model.
    """

    def __init__(self, cfg: GPT2Config):
        super().__init__()
        self.cfg = cfg

        # The token embeddings.
        self.wte = nn.Embedding(cfg.vocab_size, cfg.n_embd)

        # The position embeddings.
        self.wpe = nn.Embedding(cfg.n_positions, cfg.n_embd)

        # The dropout layer.
        self.drop = nn.Dropout(cfg.resid_pdrop)

        # The transformer blocks.
        self.h = nn.ModuleList([GPT2Block(cfg) for _ in range(cfg.n_layer)])

        # The final layer normalization.
        self.ln_f = nn.LayerNorm(cfg.n_embd, eps=cfg.layer_norm_eps)

        # Initialize all weights, use a torch rng object to be very careful.
        self.init_rng = torch.Generator()
        self.init_rng.manual_seed(42)
        self.apply(self._init_weights)

    def _init_weights(self, module):
        """
        Initialize the weights of the model.

        NOTE: The weight initialization is identical to the original OpenAI repo.
        """
        if isinstance(module, (nn.Linear, nn.Embedding)):
            nn.init.normal_(
                module.weight,
                mean=0.0,
                std=self.cfg.initializer_range,
                generator=self.init_rng,
            )
        if isinstance(module, nn.Linear) and module.bias is not None:
            nn.init.zeros_(module.bias)

    # Expose model *n_positions* so launcher can check sequence length
    @property
    def max_seq_len(self):
        return self.cfg.n_positions

    def forward(
        self,
        idx: torch.Tensor,
    ) -> torch.Tensor:
        """
        Forward pass of the model.

        Args:
            idx: The input token ids (expected to be of shape [batch, T]).

        Returns:
            The output of the model.
        """

        # Extract the batch size and sequence length from the input tensor.
        B, T = idx.size()
        if T > self.cfg.n_positions:
            raise ValueError(f"Sequence length {T} > model max {self.cfg.n_positions}")

        # Create the position embeddings.
        pos = torch.arange(0, T, dtype=torch.long, device=idx.device).unsqueeze(0)

        # Create the token embeddings.
        # NOTE: Token embeddings are of shape (b, t, n_embd).
        # NOTE: Position embeddings are of shape (t, n_embd) and will be broadcasted to
        #       (b, t, n_embd).
        x = self.wte(idx) + self.wpe(pos)

        # Apply the dropout layer.
        x = self.drop(x)

        # Apply the transformer blocks.
        for block in self.h:
            x = block(x)

        # Apply the final layer normalization.
        x = self.ln_f(x)  # [B, T, n_embd]

        return x


class GPT2(nn.Module):
    """GPT-2 backbone + language-model head with tied weights."""

    def __init__(self, cfg: GPT2Config):
        super().__init__()

        # The transformer backbone.
        self.transformer = GPT2Backbone(cfg)

        # The language model head.
        self.lm_head = nn.Linear(cfg.n_embd, cfg.vocab_size, bias=False)

        # Tie the weights of the language model head to the token embeddings.
        # NOTE: This is done to save memory by sharing the weights between the token embeddings and
        #       the language model head.
        # NOTE: This was also done in the original OpenAI repo.
        # See https://paperswithcode.com/method/weight-tying for more details.
        self.lm_head.weight = self.transformer.wte.weight

    def forward(
        self,
        idx: torch.Tensor,
        labels: Optional[torch.Tensor] = None,
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
        """
        Forward pass of the model.

        Args:
            idx: The input token ids (expected to be of shape [batch, T]).
            labels: The labels (expected to be of shape [batch, T]).

        Returns:
            The logits and the loss.
        """
        x = self.transformer(idx)  # [B, T, n_embd]
        logits = self.lm_head(x)  # [B, T, vocab]

        loss = None
        if labels is not None:
            # Shift logits and labels the standard GPT/GPT‑2 way.
            # NOTE: ignore_index is used to ignore the padding tokens.
            loss = F.cross_entropy(
                logits.view(-1, logits.size(-1)), labels.view(-1), ignore_index=-100
            )

        return logits, loss

    # TODO: Need to add checkpointing and loading from checkpoint.


if __name__ == "__main__":
    # Create a GPT-2 model configuration.
    cfg = GPT2Config(n_layer=12, n_head=12, n_embd=768)  # = GPT-2 "124 M"

    # Create a GPT-2 language model.
    model = GPT2(cfg)

    # Print the model.
    print(model)

    # Print the number of parameters in the model.
    num = sum(p.numel() for p in model.parameters()) / 1e6
    print(f"Total parameters: {num:.1f} M")  # ≈124 M
