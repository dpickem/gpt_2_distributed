{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FineWeb dataset\n",
    "\n",
    "The [üç∑ FineWeb](https://huggingface.co/datasets/HuggingFaceFW/fineweb) dataset consists of more than 15T tokens of cleaned and deduplicated english web data from CommonCrawl. The data processing pipeline is optimized for LLM performance and ran on the üè≠ datatrove library, our large scale data processing library.\n",
    "\n",
    "The dataset comes in 10B, 100B, and the full 350B token sizes. The full dataset, however, contains 51.3 TB of data, which overwhelms most local setups.\n",
    "\n",
    "In this project, we will use the 10B version, which should suffice to train a 124M GPT-2 model (as Andrej Karpathy did in [his GPT-2 tutorial](https://github.com/karpathy/llm.c/discussions/481))\n",
    "\n",
    "Along with config default (all the data), and the configs for each individual dump, you can also download the following configs:\n",
    "\n",
    "- sample-350BT: a subset randomly sampled from the whole dataset of around 350B gpt2 tokens (388GB)\n",
    "- sample-100BT: a subset randomly sampled from the whole dataset of around 100B gpt2 tokens (277.4GB)\n",
    "- sample-10BT: a subset randomly sampled from the whole dataset of around 10B gpt2 tokens (27.6GB)\n",
    "\n",
    "sample-10B was sampled from sample-100B which in turn was sampled from sample-350BT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "from typing import Any, Dict, Iterable, List, Tuple\n",
    "\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import snapshot_download\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "import torch\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetching the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e67974d4dc614addb8feefe17f94df6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 15 files:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '|Viewing Single Post From: Spoilers for the Week of February 11th|\\n|Lil||Feb 1 2013, 09:58 AM|\\nDon\\'t care about Chloe/Taniel/Jen-Jen. Don\\'t care about Sami, really, but hoping that we get some good \"SAMANTHA GENE!!\" Marlena Death-Stares out of it. And \"newfound\" feelings. Please. If only.\\nSTEFANO!! STEFANO, STEFANO, STEFANO!!!! :cheer:\\n|Spoilers for the Week of February 11th ¬∑ DAYS: News, Spoilers & Discussion|', 'id': '<urn:uuid:39147604-bfbe-4ed5-b19c-54105f8ae8a7>', 'dump': 'CC-MAIN-2013-20', 'url': 'http://daytimeroyaltyonline.com/single/?p=8906650&t=8780053', 'date': '2013-05-18T05:48:59Z', 'file_path': 's3://commoncrawl/crawl-data/CC-MAIN-2013-20/segments/1368696381249/warc/CC-MAIN-20130516092621-00000-ip-10-60-113-184.ec2.internal.warc.gz', 'language': 'en', 'language_score': 0.8232095837593079, 'token_count': 142}\n",
      "{'text': '*sigh* Fundamentalist community, let me pass on some advice to you I learned from the atheistic community:\\nIf you have set yourself on fire, do not run.\\nOkay? Okay?? Please?\\nLook, D, you had two months to say to Harvard in private emails, \"Im sorry, I shouldnt have been using that animation in my paid presentations. I wont use it again. I really do like \\'Inner Life\\', though, and would love to use it in classroom presentations, from the BioVisions site, if that is acceptable.\"\\nI sat here, for two months, waiting for that to happen, anything to happen, and it didnt. Two months, on your own terms, you could have written a similar post to yesterdays. I would have given you the benefit of the doubt-- maybe you didnt know the credits werent visible to the audience, and I wouldnt have said a word beyond this, as its Harvards problem, not mine. This would have been a funny joke to those of us involved in dealing with you people, but it would have been a PR non-issue for you.\\nBut after you set yourself on fire, you didnt douse it out with a bucket of ice cold reality and accountability. You ran. And youre still running.\\nWhy not just state \"I screwed up. Sorry everyone.\" and move on? Why the excuses? Why the denial? Why the passive language? Why the vague words and cryptic capitalizations? Why the writes and rewrites of your \\'press release\\'? We know it wasnt written of your own volition, or it would have been done *before* Harvard had to take action. And, your behavior before this, regarding this issue, is not indicative of someone who made an innocent mistake. Its weird.\\nSo what with this frantic running? Is the inability to say \"I was wrong\" a pathological feature of Creationists? Or are you hiding something? Or is it both? Or is it more?\\nAnd now we get Casey weighing in on the issue, according to cre8id at AboveTopSecret.com-- PBS/NOVA online - Intelligent Design on trial:\\n...to my knowledge, Discovery Institute has neither authorized nor received nor is making use of any presentation that used that animation. We have had nothing to do with creating or selling a DVD of that animation, nor do we have anything to do with placing that presentation on Google Video.I dont know what he is talking about with that last part, but the first part sounds similar to DIs claims post-Dover (\"WE HAD NOTHING TO DO WITH DOVER!\"). Maybe Luskin is telling the truth. Maybe this was a magic non-science Creation-friendly narration with convenient edits that AiG or ICR would have killed for... but only Dembski could find it... but he cant tell us where... and he didnt share it with anyone... and its subsequently disappeared from the Internet...\\nBut that simply isnt what Ive been told. Maybe this was all a silly Dembski mistake, blown out of proportion due to his decision to remain silent... But what if we find more videos of more DI fellows, presenting this animation?\\n*shrug* If youve set yourself on fire, do not run, DI. If youve done it, better admit it to Harvard now, apologize, and move on.', 'id': '<urn:uuid:ba819eb7-e6e6-415a-87f4-0347b6a4f017>', 'dump': 'CC-MAIN-2013-20', 'url': 'http://endogenousretrovirus.blogspot.com/2007/11/if-you-have-set-yourself-on-fire-do-not.html?showComment=1196270520000', 'date': '2013-05-18T06:43:03Z', 'file_path': 's3://commoncrawl/crawl-data/CC-MAIN-2013-20/segments/1368696381249/warc/CC-MAIN-20130516092621-00000-ip-10-60-113-184.ec2.internal.warc.gz', 'language': 'en', 'language_score': 0.9737711548805237, 'token_count': 703}\n",
      "{'text': 'A novel two-step immunotherapy approach has shown clinically beneficial responses in patients with advanced ovarian cancer. Following Lifestyle Recommendations Reduces Risk of Cancer Death\\nPeople who follow the diet and lifestyle recommendations laid out by the WCRF and the AICR have a 20 percent reduced risk of dying from cancer. UCSF Launches Social Networking Site for Patients and Families with Hereditary Cancers\\nFor Immediate Release May 14, 2013 UCSF Launches Social Networking Site for Patients and Families... Genomic Test May Help Guide Prostate Cancer Treatment\\nThe Oncotype DX¬Æ Prostate Cancer Test strongly predicts aggressiveness of disease. Statins Linked to Lower Risk of Liver Cancer in Hepatitis C\\nPeople infected with chronic hepatitis C are less likely to develop liver cancer if they are taking statins.\\nRadioimmunotherapy (RIT) is a type of targeted therapy that delivers radiation directly to cancer cells.... Urinary Incontinence\\nOverview The urinary tract includes the kidneys, the ureters, the bladder, and the urethra. The kidneys... Advanced Directives\\nLiving Wills Every competent adult has, in most cases, the freedom to accept or refuse medical treatment.... Caregivers\\nWhat is Caregiving and Who are Caregivers? Caregivers are individuals who provide care to chronically... Chemotherapy for Older Patients: What You Should Know About the Risk of Infection\\nAs you may already know, chemotherapy works by attacking the rapidly dividing cells it finds in the body,...\\nAn ongoing series highlighting complementary therapies, adapted from The Complete Guide to Complementary... Clear and precise\\nMohs surgery provides a tissue-sparing approach to skin cancer surgery. By Eleanor Mayfield Michele Kelsey... Chemical Reaction\\nChemicals may be disrupting our hormones‚Äîand our health. By Laurie Wertich Exposure to synthetic chemicals... College Kids Kick Cancer\\nBy Diana Price College kids and cancer‚Äînot two topics most of us would immediately connect. And yet... Cooking with Fruits and Vegetables\\nIn the introduction to Ripe: A Fresh, Colorful Approach to Fruits and Vegetables (Running Press, 2011;...\\nAnnual meeting brings together cancer experts from around the world. Kari Bohlke, ScD The 2011 Annual... Bone Fractures in Breast Cancer Patients More Frequent with Femara than with Tamoxifen\\nResearchers affiliated with the BIG I-98 Collaborative and International Breast Study Groups... Single Treatment with High-intensity Focused Ultrasound Effective for Localized Prostate Cancer\\nResearchers from McMaster University in Canada have reported that high-intensity focused... Marital Separation Impacts Cancer Survival\\nResearchers from the University of Indiana and the Fox Chase Cancer Center... 2009 Oncology Conference Coverage View up-to-date coverage of the 2009 Oncology Conference here.', 'id': '<urn:uuid:07b8e00d-b445-4736-a593-cd1c147dce21>', 'dump': 'CC-MAIN-2013-20', 'url': 'http://news.cancerconnect.com/', 'date': '2013-05-18T05:23:15Z', 'file_path': 's3://commoncrawl/crawl-data/CC-MAIN-2013-20/segments/1368696381249/warc/CC-MAIN-20130516092621-00000-ip-10-60-113-184.ec2.internal.warc.gz', 'language': 'en', 'language_score': 0.8727087378501892, 'token_count': 576}\n",
      "{'text': 'Free the Cans! Working Together to Reduce Waste\\nIn a blog about how people share, it‚Äôs worth the occasional reference to the bizarre ways that people DON‚ÄôT SHARE. Is it safe to say we live in a society that places great value on independence, private property, personal space, and privacy? Even sometimes extreme value? Is that why people at an 8-unit apartment building in Oakland, CA have separate caged stalls for eight separate trash cans? I know it‚Äôs not nice to stare, but I walked by these incarcerated cans and could not help myself. I returned with my camera, so that I could share my question with the world: Why can‚Äôt people share trash cans or a single dumpster? Or, at the very least, why can‚Äôt the cans share driveway space?\\nThe Zero Waste Movement has come to the Bay Area and it calls for a new use for these eight cages. Here are my suggestions:\\n- Turn two of those cages into compost bins. Fill one with grass, leaves, and vegetable scraps, let it decompose for six months, then start filling the second bin in the meantime.\\n- Put in a green can, which is what Oakland uses to collect milk cartons, pizza boxes, yard trimmings, and all food to send it to the municipal composting facility. If your city doesn‚Äôt do this yet, tell them it‚Äôs a great idea and they could be as cool and cutting edge as Oakland.\\n- Put in one or two recycling cans for glass, plastic, cardboard, paper, aluminum, etc.\\n- Put out a FREE STUFF box for unwanted clothing and household items. The neighbors could sort through it each week, and later put it out on the curb for passers-by to explore. Take what‚Äôs left to Goodwill or a comparable donation spot.\\n- Put in a few small bins for various items that can be recycled, such asbatteries and electronics, which can then be taken to an electronics recycling center every month or two. Styrofoam can be brought to a local packaging store or ceramics business that accepts used packaging material. Or, if you accumulate a bunch of plastic bags,take them to a store or to some other place that accepts used ones.\\n- Put in ONE trash can. By the time you compost, recycle, re-use, redistribute, and take a few other measures to reduce your waste, you‚Äôll have almost no trash each week.\\n- Install a bicycle rack or locked bicycle cage.\\n- With the leftover space, put in a container garden and a bench where neighbors can gather and chat. A much more pleasant alternative to the garbage can jailhouse ambiance, wouldn‚Äôt you agree?', 'id': '<urn:uuid:c970d9a2-a5ce-4050-9ea3-58d7bbd609a8>', 'dump': 'CC-MAIN-2013-20', 'url': 'http://sharingsolution.com/2009/05/23/free-the-cans-working-together-to-reduce-waste/', 'date': '2013-05-18T05:49:03Z', 'file_path': 's3://commoncrawl/crawl-data/CC-MAIN-2013-20/segments/1368696381249/warc/CC-MAIN-20130516092621-00000-ip-10-60-113-184.ec2.internal.warc.gz', 'language': 'en', 'language_score': 0.9323602318763733, 'token_count': 575}\n",
      "{'text': \"ORLANDO, Fla. ‚Äî While the Rapid Recall Exchange, the 2-year-old industry recall portal, has signed up more than 600 manufacturer and retailer subscribers, it still lacks the ‚Äúcritical mass‚Äù of suppliers that would make it a primary source of recall information, according to trade association officials and retailers.\\nManufacturers use the exchange to communicate timely and accurate product recall and withdrawal notifications to retailer and wholesaler headquarters, which in turn share the information with individual stores. The exchange's retail membership represents 85% of U.S. grocery volume ‚Äî including 21 of the 24 largest supermarket chains based in the United States ‚Äî but it still lacks key suppliers, especially in the fresh food sectors, said Pat Walsh, senior vice president, industry relations, education and research for Food Marketing Institute, Arlington, Va.\\n‚ÄúWe have good penetration [among manufacturers] on the dry grocery side ‚Äî though it needs to be better ‚Äî and need to expand in other fresh food verticals like meat, produce, deli and bakery,‚Äù said Walsh, who participated in a session on the RRE at the U Connect Live conference here earlier this month.\\nMajor food distributors in the exchange, including Kroger, Wegmans and Wakefern, have recently sent letters to their vendors explaining that the only way they will accept recall information is via the RRE, noted Brian Lynch, senior director of business and industry development for the Grocery Manufacturers Association, Washington, who also participated in the U Connect Live session. In an April letter posted on www.rapidrecallexchange.org, Kroger asked all of its suppliers to subscribe to the exchange by July 1.\\nMichael Roberson, director of corporate quality assurance for Publix Super Markets, Lakeland, Fla., said in the U Connect Live session that the chain is ‚Äúdisappointed‚Äù in the number of manufacturers using the Rapid Recall Exchange.\\n‚ÄúOnly 222 of our grocery suppliers are signed up, and more than 1,000 have not yet joined,‚Äù Roberson said. ‚ÄúWe need to have the entire food industry collaborating on the Rapid Recall Exchange.‚Äù\\nLast year, of the 300 recalls Publix experienced, fewer than 50 went through the RRE, he said, adding that industrywide only 15% of recalls were submitted to the RRE. A total of 65 recalls have been issued through the exchange industrywide since its September 2009 launch.\\nFor recalls that went through the RRE at Publix, Roberson observed ‚Äúthe absolute excellence in the information that was communicated,‚Äù including product GTINs (global trade identification numbers), the reason for the recall, and photos. ‚ÄúIf we get this information from our trading partners using RRE, then we eliminate most of the [internal] steps because everything works together through this tool,‚Äù he said. By contrast, for recalls that don't go through the RRE, ‚Äúnine times out of 10 we're going back to trading partners and seeking out additional information.‚Äù\\nPublix has been proactive in urging manufacturers to join the exchange, Roberson said. In addition, Publix has expanded its supplier scorecard to monitor and rank suppliers on whether they leverage the RRE.\\nThe RRE was created by FMI and GS1 US, Lawrenceville, N.J., which will be issuing a new version of the exchange, 2.3, in August.\", 'id': '<urn:uuid:5c2cac9e-2fda-4194-959b-6ede0668ad2a>', 'dump': 'CC-MAIN-2013-20', 'url': 'http://supermarketnews.com/food-safety/more-suppliers-sought-rapid-recall-exchange', 'date': '2013-05-18T05:25:43Z', 'file_path': 's3://commoncrawl/crawl-data/CC-MAIN-2013-20/segments/1368696381249/warc/CC-MAIN-20130516092621-00000-ip-10-60-113-184.ec2.internal.warc.gz', 'language': 'en', 'language_score': 0.9552055597305298, 'token_count': 708}\n",
      "{'text': 'September 28, 2010\\n2010 Season - Bowman pulls down CCIW honor\\n|Matt Bowman was named CCIW \"Runner of the Week\" after his fourth place finish at the Brissman-Lundeen Invitational in Rock Island, Illinois on September 24.|\\nAugustana senior Matt Bowman (Geneva HS, Elburn, Ill.) was selected as the ‚ÄúRunner of the Week‚Äù in the College Conference of Illinois & Wisconsin. Bowman‚Äôs strong performance helped the Vikings finish second at the Brissman-Lundeen Invitational at Augustana College in Rock Island, Illinois on Saturday, September 24. It was an impressive second place finish for head coach Paul Olsen‚Äôs crew as they beat four nationally ranked teams.\\nAugustana, ranked sixth in the latest U.S. Track & Field/Cross Country Coaches Association Division III Mideast Regional poll, was one of three teams ranked in the top 10 to compete at the meet. Wisconsin-Stevens Point, ranked fifth, took the team title with 23 points. Augustana finished second with 55 points while Wisconsin-Whitewater, the seventh ranked team in regional poll, placed third with 88 points. Olivet Nazarene took fourth (138), Truman State was fifth (150) and Greenville placed sixth (263).\\nThe field also included a couple of ranked teams in the Division III Central Regional poll. Cornell College, ranked ninth, finished tenth in the team scores with 307 points. Grinnell, the number one ranked team in the Central region, finished 16th with 415 points.\\nBowman led the way for Augustana with with a fourth place finish and a time of 25:10 over the 8,000 meter course. The Vikings had ten runners run a time of 26:01 or faster. Tim Thornburg of Wisconsin-Stevens Point won the individual race with a time of 24:58 while teammates Terry Witkowski and Joel Heroux finished second and third with times of 25:00 and 25:10, respectively.\\nEarlier this year, Bowman finished second overall at the Western Illinois Invitational in a time of 26:11 leading the Vikings to a team victory over a field that included Western Illinois, a Division I school. The next week Bowman was the second Viking runner to cross the line at the Illinois Intercollegiate Championships. He finished in a time of 25:49, which was good for a 26th place finish in a field made up of the top college runners in the state of Illinois.\\nAugustana, which has only lost to two NCAA Division III schools this year ‚ÄìNorth Central at the Illinois Intercollegiate meet on September 17 and this past week to Wisconsin-Stevens Point at the Brissman-Lundeen Invitational on September 24 ‚Äì will have a weekend off before they head to Waverly, Iowa to run at the Wartburg Invite on Saturday, October 9.\\nBowman, the son of Gary Bowman of Geneva, Illinois and Linda Bowman of Elburn, Illinois, is an art history major.', 'id': '<urn:uuid:a42b6d10-fba9-4678-af39-5b242fdfe790>', 'dump': 'CC-MAIN-2013-20', 'url': 'http://www.augustana.edu/x22236.xml', 'date': '2013-05-18T06:32:30Z', 'file_path': 's3://commoncrawl/crawl-data/CC-MAIN-2013-20/segments/1368696381249/warc/CC-MAIN-20130516092621-00000-ip-10-60-113-184.ec2.internal.warc.gz', 'language': 'en', 'language_score': 0.95432049036026, 'token_count': 626}\n",
      "{'text': 'Kraft Foods has taken the Cadbury chocolate brand in a new direction, by combining it with cheese for the first time.\\nThe company is bringing together two of its brands and launching Philadelphia with Cadbury, a chilled chocolate spread made from Philadelphia Light and Cadbury chocolate.\\nKraft believes the new product has the potential to do very well and is targeting ¬£10m in sales in the first year.\\nThe new cheese and chocolate spread is being launched on 1 February and will be appear in the chilled dairy aisle next to plain Philadelphia Light.\\nIt is launching in a 160g tub and a 120g four-pack of mini tubs, both with an rsp of ¬£1.62.\\nKraft is supporting the launch with a ¬£3.2m marketing budget in 2012 and is targeting 2,000 tonnes in volume sales ‚Äì equivalent to about ¬£10m ‚Äì in the first year.\\nIf they reached this volume of sales, the new Philadelphia with Cadbury would have the same market value as Garlic & Herb, currently the biggest-selling flavour in the Philadelphia portfolio.\\nKraft already offers chocolate variants of Philadelphia in Italy and Germany, using Milka chocolate and targeting the breakfast occasion.\\nIn Germany, Philadelphia with Milka has generated ‚Ç¨22.2m in sales since its October 2010 launch and has a 6.6% value share of the chocolate spread market.\\nKraft Foods UK marketing manager Bruce Newman said:\\n‚ÄúThe UK product would be positioned as a snack.\\n‚ÄúThe breakfast market in countries such as Germany is more developed, and our consumer research firmly identified Philadelphia with Cadbury as a snack.‚Äù', 'id': '<urn:uuid:8ba5fa5a-1f92-4f5a-95e3-85dbb7befbfe>', 'dump': 'CC-MAIN-2013-20', 'url': 'http://www.fdin.org.uk/2012/01/kraft-launches-new-philadelphia-with-cadbury/', 'date': '2013-05-18T05:59:16Z', 'file_path': 's3://commoncrawl/crawl-data/CC-MAIN-2013-20/segments/1368696381249/warc/CC-MAIN-20130516092621-00000-ip-10-60-113-184.ec2.internal.warc.gz', 'language': 'en', 'language_score': 0.9671564102172852, 'token_count': 337}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "datasets.iterable_dataset.IterableDataset"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the 10B token sample of the FineWeb dataset.\n",
    "local_dir = os.path.expanduser(\"~/data/datasets/fineweb/\")\n",
    "folder = snapshot_download(\n",
    "    \"HuggingFaceFW/fineweb\",\n",
    "    repo_type=\"dataset\",\n",
    "    local_dir=local_dir,\n",
    "    allow_patterns=\"sample/10BT/*\",\n",
    ")\n",
    "\n",
    "fw = load_dataset(\n",
    "    \"parquet\",\n",
    "    data_files={\"train\": f\"{local_dir}/sample/10BT/*.parquet\"},\n",
    "    streaming=True,\n",
    ")[\"train\"]\n",
    "\n",
    "for i, row in enumerate(fw):\n",
    "    print(row)\n",
    "    if i > 5:\n",
    "        break\n",
    "\n",
    "type(fw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single document tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '|Viewing Single Post From: Spoilers for the Week of February 11th|\\n|Lil||Feb 1 2013, 09:58 AM|\\nDon\\'t care about Chloe/Taniel/Jen-Jen. Don\\'t care about Sami, really, but hoping that we get some good \"SAMANTHA GENE!!\" Marlena Death-Stares out of it. And \"newfound\" feelings. Please. If only.\\nSTEFANO!! STEFANO, STEFANO, STEFANO!!!! :cheer:\\n|Spoilers for the Week of February 11th ¬∑ DAYS: News, Spoilers & Discussion|', 'id': '<urn:uuid:39147604-bfbe-4ed5-b19c-54105f8ae8a7>', 'dump': 'CC-MAIN-2013-20', 'url': 'http://daytimeroyaltyonline.com/single/?p=8906650&t=8780053', 'date': '2013-05-18T05:48:59Z', 'file_path': 's3://commoncrawl/crawl-data/CC-MAIN-2013-20/segments/1368696381249/warc/CC-MAIN-20130516092621-00000-ip-10-60-113-184.ec2.internal.warc.gz', 'language': 'en', 'language_score': 0.8232095837593079, 'token_count': 142}\n",
      "[50256    91  7680   278 14206  2947  3574    25  1338  9437   364   329\n",
      "   262  6119   286  3945  1367   400    91   198    91    43   346 15886\n",
      " 15146   352  2211    11  7769    25  3365  3001    91   198  3987   470\n",
      "  1337   546 29476    14    51  6321    14 44875    12 44875    13  2094\n",
      "   470  1337   546  3409    72    11  1107    11   475  7725   326   356\n",
      "   651   617   922   366 49302  1565  4221    32 24700    36 37160  1526\n",
      "    75  8107  5830    12  1273  3565   503   286   340    13   843   366\n",
      "  3605  9275     1  7666    13  4222    13  1002   691    13   198  2257\n",
      " 25425  1565    46  3228 24483    37  1565    46    11 24483    37  1565\n",
      "    46    11 24483    37  1565    46 13896  1058  2395   263    25   198\n",
      "    91  4561  9437   364   329   262  6119   286  3945  1367   400 14128\n",
      " 24644    50    25  3000    11  1338  9437   364  1222 27766    91]\n",
      "<class 'numpy.ndarray'> uint16\n"
     ]
    }
   ],
   "source": [
    "# Borrowed from Andrej Karpathy's GPT-2 tutorial.\n",
    "# See  https://github.com/karpathy/llm.c/blob/master/dev/data/fineweb.py\n",
    "\n",
    "\n",
    "def tokenize_gpt2(doc: Dict[str, Any]) -> np.ndarray:\n",
    "    \"\"\"Tokenizes a single document and returns a numpy array of uint16 tokens.\"\"\"\n",
    "    # Initialize the tokenizer.\n",
    "    enc = tiktoken.get_encoding(\"gpt2\")\n",
    "    encode = lambda s: enc.encode_ordinary(s)\n",
    "\n",
    "    # Define the end of text token.\n",
    "    eot = enc._special_tokens[\"<|endoftext|>\"]  # end of text token\n",
    "\n",
    "    # The special <|endoftext|> token delimits all documents.\n",
    "    # NOTE: The end of text token is prepended to the document (rather than appended).\n",
    "    tokens = [eot]\n",
    "\n",
    "    # Encode the document.\n",
    "    tokens.extend(encode(doc[\"text\"]))\n",
    "    tokens_np = np.array(tokens)\n",
    "\n",
    "    # Check that the tokens are within the range of uint16.\n",
    "    assert (0 <= tokens_np).all() and (\n",
    "        tokens_np < 2**16\n",
    "    ).all(), \"Token dictionary too large for uint16\"\n",
    "\n",
    "    # Convert the tokens to a numpy array of uint16 tokens.\n",
    "    tokens_np_uint = tokens_np.astype(np.uint16)\n",
    "    return tokens_np_uint\n",
    "\n",
    "\n",
    "def decode_gpt2(tokens: torch.Tensor) -> str:\n",
    "    \"\"\"Decodes a tensor of uint16 tokens to a string.\"\"\"\n",
    "    enc = tiktoken.get_encoding(\"gpt2\")\n",
    "    return enc.decode(tokens.tolist())\n",
    "\n",
    "\n",
    "for i, row in enumerate(fw):\n",
    "    print(row)\n",
    "    tokens = tokenize_gpt2(row)\n",
    "    print(tokens)\n",
    "    print(type(tokens), tokens.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data file writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADERS_INFO = {\n",
    "    \"gpt-2\": {\n",
    "        \"magic\": 20240520,\n",
    "        \"version\": 1,\n",
    "        \"token_dtype\": np.uint16,\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def write_token_shard_uint16_to_json(\n",
    "    tokens: np.ndarray,\n",
    "    out_path: pathlib.Path,\n",
    ") -> None:\n",
    "    \"\"\"Write token data to a .json file for easier handling in the data loader.\n",
    "\n",
    "    Args:\n",
    "        tokens: The tokens to write to the file (as a numpy array of uint16).\n",
    "        out_path: The path to the file to write to.\n",
    "    \"\"\"\n",
    "    assert len(tokens) < 2**31, \"token count too large\"  # ~2.1B tokens\n",
    "\n",
    "    # Construct the header.\n",
    "    info = HEADERS_INFO[\"gpt-2\"]\n",
    "    data = {\n",
    "        \"header\": {\n",
    "            \"model\": \"gpt-2\",\n",
    "            \"magic\": info[\"magic\"],\n",
    "            \"version\": info[\"version\"],\n",
    "            \"num_tokens\": len(tokens),\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Construct the data (numpy array of tokens).\n",
    "    data[\"tokens\"] = tokens.tolist()\n",
    "\n",
    "    # Write to file.\n",
    "    print(f\"Writing {len(tokens):,} tokens to {out_path} in the gpt-2 format\")\n",
    "    with open(out_path, \"w\") as f:\n",
    "        json.dump(data, f)\n",
    "\n",
    "\n",
    "# write_bin_shard.py\n",
    "def write_token_shard_uint16_to_bin(\n",
    "    tokens: Iterable[np.ndarray | list[int]],\n",
    "    out_path: pathlib.Path,\n",
    "    *,\n",
    "    chunk_size: int = 1_000_000,\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Convert a stream/array of uint16 GPT-style tokens to a contiguous .bin\n",
    "    file with *little-endian uint16* entries.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tokens       : Either a single np.ndarray / list of uint16s, or an\n",
    "                   **iterator** that yields many such chunks.\n",
    "    out_path     : Destination *.bin* file.\n",
    "    chunk_size   : If `tokens` is a single ndarray, we still slice it\n",
    "                   into <=chunk_size pieces to keep peak RAM small.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int - number of tokens written (so you can update metadata.json).\n",
    "    \"\"\"\n",
    "    # Create the parent directory if it doesn't exist.\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # open in binary write‚Äëmode once, stream successive chunks\n",
    "    with out_path.open(\"wb\") as f:\n",
    "        total = 0\n",
    "\n",
    "        def _writer(chunk: np.ndarray):\n",
    "            \"\"\"Write a chunk of tokens to the file.\"\"\"\n",
    "            # Required to modify the 'total' variable from outer scope.\n",
    "            nonlocal total\n",
    "\n",
    "            # Convert the chunk to uint16 if it's not already.\n",
    "            assert chunk.dtype == np.uint16, \"chunk datatype must be uint16\"\n",
    "\n",
    "            # Cast to uint16 little-endian *without* changing the underlying values.\n",
    "            view16 = chunk.astype(\"<u2\", copy=False)\n",
    "            view16.tofile(f)\n",
    "            total += len(view16)\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # branch: `tokens` is already an iterator vs. a monolithic array\n",
    "        if isinstance(tokens, (list, np.ndarray)):\n",
    "            arr = np.asarray(tokens, dtype=np.uint16)\n",
    "            for i in range(0, len(arr), chunk_size):\n",
    "                _writer(arr[i : i + chunk_size])\n",
    "        else:\n",
    "            # user supplied an iterator / generator of chunks\n",
    "            for chunk in tokens:\n",
    "                _writer(np.asarray(chunk, dtype=np.uint16))\n",
    "\n",
    "    return total\n",
    "\n",
    "\n",
    "test_filename = pathlib.Path(\n",
    "    os.path.expanduser(\n",
    "        \"~/data/datasets/fineweb/sample/10BT_tokenized/fineweb_sample_10BT_train_test.bin\"\n",
    "    )\n",
    ")\n",
    "write_token_shard_uint16_to_bin(tokens=tokenize_gpt2(row), out_path=test_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify the binary file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = np.memmap(test_filename, dtype=\"<u2\", mode=\"r\")\n",
    "assert mm.dtype == np.uint16 and mm[0] < 65_536\n",
    "print(f\"{len(mm)} tokens loaded via memmap: {mm[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full dataset tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a helper function for file writing.\n",
    "def write_data_to_file(\n",
    "    filename_without_extension: pathlib.Path, data: np.ndarray, file_type: str\n",
    ") -> None:\n",
    "    \"\"\"Write data to a file.\"\"\"\n",
    "    # Create the parent directory if it doesn't exist.\n",
    "    filename_without_extension.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if file_type == \"json\":\n",
    "        filename = filename_without_extension.with_suffix(\".json\")\n",
    "        write_token_shard_uint16_to_json(tokens=data, out_path=filename)\n",
    "    elif file_type == \"bin\":\n",
    "        filename = filename_without_extension.with_suffix(\".bin\")\n",
    "        write_token_shard_uint16_to_bin(tokens=data, out_path=filename)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid file type: {file_type}\")\n",
    "\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize all documents and write output shards, each of shard_size tokens (last shard has\n",
    "# remainder)\n",
    "\n",
    "# Set the shard size (size of each data shard in the output .json files, in tokens).\n",
    "SHARD_SIZE = 10**8  # 100M tokens\n",
    "\n",
    "# Set the token dtype.\n",
    "TOKEN_DTYPE = np.uint16\n",
    "\n",
    "# Define the dataset name.\n",
    "DATASET_NAME = \"fineweb_sample_10BT\"\n",
    "\n",
    "# Define the data cache directory.\n",
    "DATA_CACHE_DIR = os.path.expanduser(\"~/data/datasets/fineweb/sample/10BT_tokenized\")\n",
    "\n",
    "# Allocate the number of processes to use (N - 2 to avoid hogging the entire system).\n",
    "N_PROCS = max(1, os.cpu_count() - 2)\n",
    "\n",
    "# Define the file type.\n",
    "FILE_TYPE = \"bin\"\n",
    "\n",
    "# Instantiate the metadata.json file.\n",
    "metadata = {\n",
    "    \"dataset_name\": \"fineweb_sample_10BT\",\n",
    "    \"data_cache_dir\": os.path.expanduser(\n",
    "        \"~/data/datasets/fineweb/sample/10BT_tokenized\"\n",
    "    ),\n",
    "    \"metadata\": {\n",
    "        \"train\": [],\n",
    "        \"val\": [],\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def get_filename(shard_index: int) -> pathlib.Path:\n",
    "    split = \"val\" if shard_index == 0 else \"train\"\n",
    "    return pathlib.Path(\n",
    "        os.path.join(DATA_CACHE_DIR, f\"{DATASET_NAME}_{split}_{shard_index:06d}\")\n",
    "    )\n",
    "\n",
    "\n",
    "process = True\n",
    "if process:\n",
    "    with mp.Pool(N_PROCS) as pool:\n",
    "        # Initialize the shard index to 0.\n",
    "        shard_index = 0\n",
    "\n",
    "        # Preallocate buffer to hold current shard.\n",
    "        all_tokens_np = np.empty((SHARD_SIZE,), dtype=TOKEN_DTYPE)\n",
    "        token_count = 0\n",
    "        progress_bar = None\n",
    "\n",
    "        for tokens in pool.imap(tokenize_gpt2, fw, chunksize=16):\n",
    "\n",
    "            # Is there enough space in the current shard for the new tokens?\n",
    "            if token_count + len(tokens) < SHARD_SIZE:\n",
    "                # Simply append tokens to current shard.\n",
    "                all_tokens_np[token_count : token_count + len(tokens)] = tokens\n",
    "                token_count += len(tokens)\n",
    "\n",
    "                # Update progress bar.\n",
    "                if progress_bar is None:\n",
    "                    progress_bar = tqdm(\n",
    "                        total=SHARD_SIZE, unit=\"tokens\", desc=f\"Shard {shard_index}\"\n",
    "                    )\n",
    "                progress_bar.update(len(tokens))\n",
    "            else:\n",
    "                # Write the current shard and start a new one.\n",
    "                filename = get_filename(shard_index)\n",
    "\n",
    "                # Split the document into whatever fits in this shard, the remainder goes to next one.\n",
    "                remainder = SHARD_SIZE - token_count\n",
    "                progress_bar.update(remainder)\n",
    "                all_tokens_np[token_count : token_count + remainder] = tokens[\n",
    "                    :remainder\n",
    "                ]\n",
    "\n",
    "                write_data_to_file(filename, all_tokens_np, FILE_TYPE)\n",
    "                shard_index += 1\n",
    "                progress_bar = None\n",
    "\n",
    "                # Populate the next shard with the leftovers of the current doc.\n",
    "                all_tokens_np[0 : len(tokens) - remainder] = tokens[remainder:]\n",
    "                token_count = len(tokens) - remainder\n",
    "\n",
    "        # Write any remaining tokens as the last shard.\n",
    "        if token_count != 0:\n",
    "            filename = get_filename(shard_index)\n",
    "            write_data_to_file(filename, all_tokens_np[:token_count], FILE_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize metadata.\n",
    "metadata = {\n",
    "    \"dataset_name\": \"fineweb_sample_10BT\",\n",
    "    \"data_cache_dir\": os.path.expanduser(\n",
    "        \"~/data/datasets/fineweb/sample/10BT_tokenized\"\n",
    "    ),\n",
    "    \"metadata\": {\n",
    "        \"train\": [],\n",
    "        \"val\": [],\n",
    "    },\n",
    "}\n",
    "\n",
    "# Get a list of all pre-processed data shards in the DATA_CACHE_DIR.\n",
    "DATA_CACHE_DIR = os.path.expanduser(\"~/data/datasets/fineweb/sample/10BT_tokenized\")\n",
    "for file_path in tqdm(\n",
    "    sorted(\n",
    "        [\n",
    "            os.path.join(DATA_CACHE_DIR, f)\n",
    "            for f in os.listdir(DATA_CACHE_DIR)\n",
    "            if f.endswith(\".json\")\n",
    "        ]\n",
    "    ),\n",
    "    desc=\"Loading data shards\",\n",
    "):\n",
    "    # Load the data shard.\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Get the number of tokens in the data shard.\n",
    "    header = data[\"header\"]\n",
    "    header[\"filename\"] = os.path.basename(file_path)\n",
    "\n",
    "    if \"train\" in file_path:\n",
    "        metadata[\"metadata\"][\"train\"].append(header)\n",
    "    elif \"val\" in file_path:\n",
    "        metadata[\"metadata\"][\"val\"].append(header)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown file type: {file_path}\")\n",
    "\n",
    "metadata\n",
    "\n",
    "# Save the metadata to a JSON file.\n",
    "with open(os.path.join(DATA_CACHE_DIR, \"metadata.json\"), \"w\") as f:\n",
    "    json.dump(metadata, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a shard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = pathlib.Path(\n",
    "    os.path.expanduser(\n",
    "        \"~/data/datasets/fineweb/sample/10BT_tokenized/fineweb_sample_10BT_train_000001.json\"\n",
    "    )\n",
    ")\n",
    "\n",
    "with open(filename, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(data.keys())\n",
    "print(data[\"header\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000000 tokens loaded via memmap: [9370 4146 2662 2767 1581]\n"
     ]
    }
   ],
   "source": [
    "filename = pathlib.Path(\n",
    "    os.path.expanduser(\n",
    "        \"~/data/datasets/fineweb/sample/10BT_tokenized/fineweb_sample_10BT_train_000001.bin\"\n",
    "    )\n",
    ")\n",
    "\n",
    "mm = np.memmap(filename, dtype=\"<u2\", mode=\"r\")\n",
    "assert mm.dtype == np.uint16 and mm[0] < 65_536\n",
    "print(f\"{len(mm)} tokens loaded via memmap: {mm[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset and Dataloader\n",
    "\n",
    "## Design choices\n",
    "\n",
    "The following table summarizes the design choices made for this data set and loader (by OpenAI's O3-Pro model).\n",
    "\n",
    "| Pain point                                    | Design choice                                                                         | Ref                |\n",
    "| --------------------------------------------- | ------------------------------------------------------------------------------------- | ------------------ |\n",
    "| **RAM blow‚Äëup**                               | `np.memmap` ‚ûú pages are faulted in *on demand*, O/S handles LRU                       | ([comet.com][1])   |\n",
    "| **10‚ÄØB tokens won‚Äôt fit in a Python list**    | `IterableDataset` yields one sample at a time, no `__len__` or index repo needed      | ([pytorch.org][2]) |\n",
    "| **Duplicate data across  n‚ÄØ√ó‚ÄØgpus‚ÄØ√ó‚ÄØworkers** | stride¬†`start::stride` partitions the shard list deterministically by `<rank,worker>` | ‚Äì                  |\n",
    "| **Epoch‚Äëwise shuffling**                      | Shuffle order of shards *and* intra‚Äëshard offsets with epoch‚Äëseeded RNG               | ‚Äì                  |\n",
    "\n",
    "[1]: https://www.comet.com/site/blog/understanding-memory-mapping-in-numpy-for-deep-learning-pt-1/?utm_source=chatgpt.com \"Understanding Memory Mapping in Numpy for Deep Learning: Pt 1\"\n",
    "[2]: https://pytorch.org/docs/stable/data.html?utm_source=chatgpt.com \"torch.utils.data - PyTorch documentation\"\n",
    "\n",
    "## Why is this distributed-safe\n",
    "\n",
    "| DDP/FSDP requirement                    | How the dataset meets it                                                                                                                                        |\n",
    "| --------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Every rank sees different data**      | `stride = world‚ÄØ√ó‚ÄØworkers` slices shard list by `(rank,worker)`                                                                                                 |\n",
    "| **Repeatable shuffling**                | seed¬†=¬†`base_seed + epoch` ‚áí deterministic order per epoch                                                                                                      |\n",
    "| **Equal *number* of samples per rank**  | shards are equal‚Äësize (100‚ÄØM tokens); last shard is shorter but still split identically; uneven leftovers are dropped automatically when the generator exhausts |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 103 training data files in /home/dpickem/data/datasets/fineweb/sample/10BT_tokenized\n",
      "/home/dpickem/data/datasets/fineweb/sample/10BT_tokenized/fineweb_sample_10BT_train_000001.bin\n",
      "/home/dpickem/data/datasets/fineweb/sample/10BT_tokenized/fineweb_sample_10BT_train_000002.bin\n",
      "/home/dpickem/data/datasets/fineweb/sample/10BT_tokenized/fineweb_sample_10BT_train_000003.bin\n",
      "/home/dpickem/data/datasets/fineweb/sample/10BT_tokenized/fineweb_sample_10BT_train_000004.bin\n",
      "/home/dpickem/data/datasets/fineweb/sample/10BT_tokenized/fineweb_sample_10BT_train_000005.bin\n"
     ]
    }
   ],
   "source": [
    "import os, random, numpy as np, torch, pathlib\n",
    "from typing import Iterator, List, Tuple\n",
    "from torch.utils.data import IterableDataset, get_worker_info, DataLoader\n",
    "import torch.distributed as dist\n",
    "\n",
    "\n",
    "class TokenShardDataset(IterableDataset):\n",
    "    \"\"\"\n",
    "    Streams fixed‚Äëlength training examples from many >RAM shards\n",
    "    without ever loading a full shard into memory.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, shard_paths: List[pathlib.Path], seq_len: int = 1024, shuffle: bool = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "\n",
    "        Args:\n",
    "            shard_paths: List of paths to the shard files.\n",
    "            seq_len: The length of the sequence to yield.\n",
    "            shuffle: Whether to shuffle the shards.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.shard_paths = list(shard_paths)\n",
    "        self.seq_len = seq_len\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        # Discover world context once so every epoch can reuse it.\n",
    "        self.rank, self.world = (\n",
    "            (dist.get_rank(), dist.get_world_size())\n",
    "            if dist.is_initialized()\n",
    "            else (0, 1)\n",
    "        )\n",
    "        print(f\"Rank: {self.rank}, World: {self.world}\")\n",
    "\n",
    "    # ------------------------------------------------------------------ helpers\n",
    "    def _open_shard(self, path: pathlib.Path) -> Tuple[np.ndarray, int]:\n",
    "        \"\"\"\n",
    "        Open a shard file and return a memory-mapped array of the tokens.\n",
    "\n",
    "        NOTE: The shard files are memory-mapped as unsigned 16-bit (2-byte) little-endian (as\n",
    "              generated by the `tokenize_gpt2` function in this notebook).\n",
    "\n",
    "        Args:\n",
    "            path: Path to the shard file.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[np.ndarray, int]: The memory-mapped array of the tokens and the number of tokens.\n",
    "        \"\"\"\n",
    "        # Memory‚Äëmap as unsigned 16‚Äëbit (2‚ÄØbytes) little‚Äëendian.\n",
    "        n_bytes = os.path.getsize(path)\n",
    "        n_tokens = n_bytes // 2\n",
    "        mm = np.memmap(path, dtype=\"<u2\", mode=\"r\", shape=(n_tokens,))\n",
    "        return mm, n_tokens\n",
    "\n",
    "    def _iter_one_shard(\n",
    "        self, mm: np.ndarray, n_tokens: int\n",
    "    ) -> Iterator[Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Iterate over a shard file and yield (input, target) pairs.\n",
    "\n",
    "        Args:\n",
    "            mm: The memory-mapped array of the tokens.\n",
    "            n_tokens: The number of tokens in the shard.\n",
    "        \"\"\"\n",
    "        # +1 because we need (seq_len+1) to create (input, target) pairs.\n",
    "        max_offset = n_tokens - (self.seq_len + 1)\n",
    "        if max_offset <= 0:\n",
    "            return  # Skip shards shorter than a context.\n",
    "\n",
    "        # A deterministic but different start per GPU & worker.\n",
    "        g = random.Random()  # Local RNG; thread‚Äësafe.\n",
    "        seed = (self.epoch * 17) ^ (self.rank * 971) ^ (self.worker_id * 31)\n",
    "        g.seed(seed)\n",
    "\n",
    "        # Streaming: walk through the shard once each epoch.\n",
    "        offsets = list(range(0, max_offset, self.seq_len))\n",
    "        if self.shuffle:\n",
    "            g.shuffle(offsets)\n",
    "\n",
    "        for off in offsets:\n",
    "            seq = np.array(mm[off : off + self.seq_len + 1], copy=False)\n",
    "            x = torch.from_numpy(seq[:-1])  # Input ids.\n",
    "            y = torch.from_numpy(seq[1:])  # Targets.\n",
    "            yield x.long(), y.long()\n",
    "\n",
    "    # ------------------------------------------------------------------ main iterator\n",
    "    def __iter__(self) -> Iterator[Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Iterate over the dataset.\n",
    "        \"\"\"\n",
    "        self.worker_id = 0\n",
    "        if (info := get_worker_info()) is not None:\n",
    "            self.worker_id = info.id\n",
    "            self.num_workers = info.num_workers\n",
    "        else:\n",
    "            self.num_workers = 1\n",
    "        self.epoch = getattr(self, \"_epoch\", 0)  # set by the DataLoader\n",
    "\n",
    "        # slice the shard list: each <rank,worker> gets every N‚Äëth file\n",
    "        shards = self.shard_paths.copy()\n",
    "        if self.shuffle:\n",
    "            random.Random(self.epoch).shuffle(shards)\n",
    "\n",
    "        # unique stride = world * num_workers\n",
    "        stride = self.world * self.num_workers\n",
    "        start = self.rank * self.num_workers + self.worker_id\n",
    "        my_shards = shards[start::stride]\n",
    "\n",
    "        for path in my_shards:\n",
    "            mm, n_tokens = self._open_shard(path)\n",
    "            yield from self._iter_one_shard(mm, n_tokens)\n",
    "\n",
    "    def set_epoch(self, epoch: int):\n",
    "        \"\"\"\n",
    "        Set the epoch number.\n",
    "\n",
    "        NOTE: The `DataLoader` calls this hook at the end of each epoch.\n",
    "\n",
    "        Args:\n",
    "            epoch: The epoch number.\n",
    "        \"\"\"\n",
    "        self._epoch = epoch\n",
    "\n",
    "\n",
    "# Get a list of all pre-processed data shards in the DATA_CACHE_DIR.\n",
    "DATA_CACHE_DIR = os.path.expanduser(\"~/data/datasets/fineweb/sample/10BT_tokenized\")\n",
    "file_paths = sorted(\n",
    "    [\n",
    "        os.path.join(DATA_CACHE_DIR, f)\n",
    "        for f in os.listdir(DATA_CACHE_DIR)\n",
    "        if f.endswith(\".bin\") and \"train\" in f\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Print the number of files.\n",
    "print(f\"Found {len(file_paths)} training data files in {DATA_CACHE_DIR}\")\n",
    "for f in file_paths[:5]:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common pitfalls\n",
    "\n",
    "| Symptom                                                               | Fix                                                                                                  |\n",
    "| --------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------- |\n",
    "| **‚ÄúToo many open files‚Äù** after a few epochs                          | `ulimit -n 4096` or open shards lazily as shown rather than in `__init__`.                           |\n",
    "| **Imbalanced batches across ranks** (some GPUs finish an epoch early) | Make sure every shard is *exactly* the same length or pad the last sequence to `seq_len+1`.          |\n",
    "| CUDA error at first all‚Äëreduce                                        | Confirm every rank sees the *same* batch size (no drop‚Äëlast) and `dist.barrier()` after `set_epoch`. |\n",
    "| Slow first epoch, fast later                                          | Normal: the kernel page‚Äëfaults the first pass, then serves from cache.                               |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank: 0, World: 1\n"
     ]
    }
   ],
   "source": [
    "# Default batch size. This determines how many sequences are processed together in each iteration.\n",
    "DEFAULT_BATCH_SIZE = 16\n",
    "\n",
    "# Default context length. This determines the maximum length of the input sequences (1024 is\n",
    "# the maximum context length supported by our GPT-2 model).\n",
    "DEFAULT_CONTEXT_LENGTH = 1024\n",
    "\n",
    "# Number of subprocesses for data loading. This enables parallel data loading using multiple\n",
    "# worker processes\n",
    "N_PROCS = 2\n",
    "\n",
    "# Initialize the dataset.\n",
    "ds = TokenShardDataset(shard_paths=file_paths, seq_len=DEFAULT_CONTEXT_LENGTH)\n",
    "\n",
    "# Initialize the data loader.\n",
    "dl = torch.utils.data.DataLoader(\n",
    "    ds,\n",
    "    batch_size=DEFAULT_BATCH_SIZE,\n",
    "    num_workers=N_PROCS,\n",
    "    # pin_memory: If True, the data loader will copy tensors into CUDA pinned memory\n",
    "    #             This speeds up GPU transfer by using page-locked memory that can't be swapped out\n",
    "    pin_memory=True,\n",
    "    # prefetch_factor: Number of batches loaded in advance by each worker\n",
    "    #                  Here set to 2, meaning each worker queues 2 batches ahead\n",
    "    #                  This helps maintain a steady flow of data to the GPU\n",
    "    prefetch_factor=2,\n",
    "    # persistent_workers: If True, worker processes are not killed between epochs\n",
    "    #                     This avoids the overhead of recreating workers for each epoch\n",
    "    persistent_workers=True,\n",
    "    # drop_last: If True, drops the last incomplete batch if dataset size is not divisible by\n",
    "    #            batch_size. This ensures all batches have the same size, which is often required\n",
    "    #            for training.\n",
    "    drop_last=True,\n",
    "    # collate_fn: Custom function to merge a list of samples into a batch\n",
    "    #             Here it takes a batch of (data, labels) tuples and stacks them into tensors\n",
    "    #             The lambda function: lambda batch: tuple(torch.stack(x) for x in zip(*batch))\n",
    "    #             - zip(*batch) groups all data samples together and all label samples together\n",
    "    #             - torch.stack(x) converts each group into a single tensor\n",
    "    #             - tuple(...) returns the stacked data and labels as a tuple\n",
    "    collate_fn=lambda batch: tuple(torch.stack(x) for x in zip(*batch)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0:\n",
      "  data.shape: torch.Size([16, 1024])\n",
      "  labels.shape: torch.Size([16, 1024])\n",
      "  data: tensor([[  366,  1462, 12475,  ...,   606,   625,   612],\n",
      "        [42791, 42693,    11,  ...,    89, 23641,  1722],\n",
      "        [  250, 27903, 10211,  ...,    13, 10127,   345],\n",
      "        ...,\n",
      "        [ 1532,   612,   447,  ...,  4838,    13,   314],\n",
      "        [  509,  1040,   805,  ...,   389, 26731,  2004],\n",
      "        [  737, 16227,    11,  ...,    82,   737, 20401]])\n",
      "  labels: tensor([[ 1462, 12475,   262,  ...,   625,   612,    13],\n",
      "        [42693,    11,   290,  ..., 23641,  1722,    13],\n",
      "        [27903, 10211, 12840,  ..., 10127,   345,   821],\n",
      "        ...,\n",
      "        [  612,   447,   247,  ...,    13,   314,  2497],\n",
      "        [ 1040,   805,  2297,  ..., 26731,  2004,   284],\n",
      "        [16227,    11, 10812,  ...,   737, 20401,    12]])\n"
     ]
    }
   ],
   "source": [
    "for step, (data, labels) in enumerate(dl):\n",
    "    print(f\"Step {step}:\")\n",
    "    print(f\"  data.shape: {data.shape}\")\n",
    "    print(f\"  labels.shape: {labels.shape}\")\n",
    "    print(f\"  data: {data}\")\n",
    "    print(f\"  labels: {labels}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "). Hence, genes in multiple-gene systems are called quantitative trait loci (QTLs). The objective of QTL study is to discover multiple genes with different effect sizes, contributing to the variation of the trait. Improved QTL linkage studies, which use many small families instead of few large ones, can be used to study the extremes of a dimension and are capable of finding genes with more than 10% effect size . Cardon et al. identified the first QTL linkage, a linkage for reading disability.\n",
      "QTLs can also be identified with a simpler method that can detect QTLs with even much smaller effect sizes on the variance of the trait. This method is called allelic association and detects the correlation between a specific allele and trait in the population.\n",
      "Benjamin et al.  reported one of the first associations for personality: the association between dopamine D4 receptor (DRD4), and novelty seeking. DRD4 gene has two types of alleles, short and long. Novelty seeking scores were higher in subjects with long DRD4 alleles. However, long alleles are also associated with hyperactivity . Association methods have also been used to study diseases such as Alzheimer‚Äôs disease . The allele associated with this disease is apolipoprotein E (APOE)-4.\n",
      "QTLs associated with intelligence are being investigated under the IQ QTL Project. The project focuses on ability rather than disability. The hypothesis of this project is that being very high functioning calls for most of the positive alleles and hardly any of the negative ones. The aim is to use very high functioning individuals to identify QTLs that work all through the total distribution6. Studies of complex traits such as intelligence have 80% power to detect QTLs when they are responsible for 1% heritability. This means that when a single marker is studied, the size of an unselected sample should be about 800. Some of the studies that have used such a large-scale sample size of unselected individuals have reported positive association between normal variation in g and candidate genes including Cathepsin D (CTSD) and cholinergic muscarinic 2 receptor (CHRM2) [16,17] , and the association of catechol-o-methyltransferase gene (COMT) with working memory [18,19]. The studies need to be replicated: Wickelgren et al. reported a positive association between insulin-like growth factor receptor gene with learning and memory but using a larger sample Hill et al. did not find such an association. Moreover, there are many QTLs that are responsible for less than 1% heritability, which means detecting them is difficult.\n",
      "While genomics provides information about what proteins could potentially be in a cell, only functional genomics determines which of those proteins actually exist. Proteins also undergo modifications and interactions that cannot be predicted from genomics alone. Functional genomics operates under three major categories: gene manipulation, gene expression profiling, and proteomics.\n",
      "In gene manipulation studies the sequence of a gene of interest in an animal model can be deleted, ‚Äúknocking out‚Äù the gene. This method has been used in intelligence related studies. Silva et al. , for example, found that knocking out a particular kinase gene interferes with mouse spatial tasks. Another approach involves ‚Äúknocking down‚Äù the genes using antisense DNA, which binds a complimentary RNA and prevents its translation. Using this approach, Guzowski and McGaugh  reported the importance of CRE-binding protein (CREB) gene in memory formation. While knocking out or knocking down a gene can affect a behavior this does not mean that the gene causes that behavior: it is possible the gene along with hundreds of others is involved. Conversely, redundancy in living systems can result in little effect when genes are knocked down and out. Moreover, since different knock out strains can show different results, the importance of other genes is evident. Lastly, knocking out or knocking down genes of interest does not exactly parallel natural control behavior.\n",
      "Gene expression can be determined by the presence of mRNA which could be transcribed into proteins. Microarrays can detect the expression of many genes at the same time. Gene expression depends on the tissue that is sampled, making the approach difficult for human studies because brain tissue is required. Nevertheless, gene expression profiling has been broadly used in mice.\n",
      "Proteomics is the study the entire protein complement (proteome) of a cell, tissue or organism. The level of mRNA expression is not always a good reflection of protein level, thus proteomics yields a much more accurate snapshot of a cell‚Äôs proteome.\n",
      "There remains a lack of scientific precision for the definition of intelligence, although many scientists use the psychometric definition, g (general cognitive ability). Whether nature or nurture influences intelligence remains a matter of debate between geneticists and environmentalists, and sits at about 50/50. Intelligence appears to be controlled in part by quantitative trait loci (QTLs). Multiple\n"
     ]
    }
   ],
   "source": [
    "print(decode_gpt2(tokens=data[15]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt_2_distributed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
