"""
Dataloader for the token shard dataset.
"""

import os
import random
import numpy as np
import pathlib
from typing import Iterator, List, Tuple

import torch
from torch.utils import data as torch_data
import torch.distributed as dist

# Default batch size. This determines how many sequences are processed together in each iteration.
# NOTE: A batch size of 16 resulted in OOM errors on a NVIDIA RTX 5000 with 32GB of memory.
DEFAULT_BATCH_SIZE = 4  # Reduced from 16 to fit in memory

# Default context length. This determines the maximum length of the input sequences (1024 is
# the maximum context length supported by our GPT-2 model).
DEFAULT_CONTEXT_LENGTH = 1024

# Number of subprocesses for data loading. This enables parallel data loading using multiple
# worker processes
DEFAULT_N_PROCS = 2

# Default prefetch factor. The number of batches loaded in advance by each worker.
DEFAULT_PREFETCH_FACTOR = 2


def get_shard_paths(
    data_cache_dir: pathlib.Path, split: str = "train", file_type: str = "bin"
) -> List[pathlib.Path]:
    """
    Get a list of all pre-processed data shards in the DATA_CACHE_DIR.

    Args:
        data_cache_dir: The directory containing the data shards.
        split: The split of the data to get the shard paths for.
        file_type: The type of file to get the shard paths for.

    Returns:
        List[pathlib.Path]: A list of paths to the shard files.
    """
    return sorted(
        [
            data_cache_dir / f
            for f in os.listdir(data_cache_dir)
            if f.endswith(f".{file_type}") and split in f
        ]
    )


class TokenShardDataset(torch_data.IterableDataset):
    """
    Streams fixed‑length training examples from many >RAM shards
    without ever loading a full shard into memory.
    """

    def __init__(
        self, shard_paths: List[pathlib.Path], seq_len: int = 1024, shuffle: bool = True
    ):
        """
        Initialize the dataset.

        Args:
            shard_paths: List of paths to the shard files.
            seq_len: The length of the sequence to yield.
            shuffle: Whether to shuffle the shards.
        """
        super().__init__()
        self.shard_paths = list(shard_paths)
        self.seq_len = seq_len
        self.shuffle = shuffle

        # Discover world context once so every epoch can reuse it.
        self.rank, self.world = (
            (dist.get_rank(), dist.get_world_size())
            if dist.is_initialized()
            else (0, 1)
        )
        print(f"Rank: {self.rank}, World: {self.world}")

    # ------------------------------------------------------------------ helpers
    def _open_shard(self, path: pathlib.Path) -> Tuple[np.ndarray, int]:
        """
        Open a shard file and return a memory-mapped array of the tokens.

        NOTE: The shard files are memory-mapped as unsigned 16-bit (2-byte) little-endian (as
              generated by the `tokenize_gpt2` function in this notebook).

        Args:
            path: Path to the shard file.

        Returns:
            Tuple[np.ndarray, int]: The memory-mapped array of the tokens and the number of tokens.
        """
        # Memory‑map as unsigned 16‑bit (2 bytes) little‑endian.
        n_bytes = os.path.getsize(path)
        n_tokens = n_bytes // 2
        mm = np.memmap(path, dtype="<u2", mode="r", shape=(n_tokens,))
        return mm, n_tokens

    def _iter_one_shard(
        self, mm: np.ndarray, n_tokens: int
    ) -> Iterator[Tuple[torch.Tensor, torch.Tensor]]:
        """
        Iterate over a shard file and yield (input, target) pairs.

        Args:
            mm: The memory-mapped array of the tokens.
            n_tokens: The number of tokens in the shard.
        """
        # +1 because we need (seq_len+1) to create (input, target) pairs.
        max_offset = n_tokens - (self.seq_len + 1)
        if max_offset <= 0:
            return  # Skip shards shorter than a context.

        # A deterministic but different start per GPU & worker.
        g = random.Random()  # Local RNG; thread‑safe.
        seed = (self.epoch * 17) ^ (self.rank * 971) ^ (self.worker_id * 31)
        g.seed(seed)

        # Streaming: walk through the shard once each epoch.
        offsets = list(range(0, max_offset, self.seq_len))
        if self.shuffle:
            g.shuffle(offsets)

        for off in offsets:
            seq = np.array(mm[off:off + self.seq_len + 1], copy=True)
            x = torch.from_numpy(seq[:-1])  # Input ids.
            y = torch.from_numpy(seq[1:])  # Targets.
            yield x.long(), y.long()

    # ------------------------------------------------------------------ main iterator
    def __iter__(self) -> Iterator[Tuple[torch.Tensor, torch.Tensor]]:
        """
        Iterate over the dataset.
        """
        self.worker_id = 0
        if (info := torch_data.get_worker_info()) is not None:
            self.worker_id = info.id
            self.num_workers = info.num_workers
        else:
            self.num_workers = 1
        self.epoch = getattr(self, "_epoch", 0)  # set by the DataLoader

        # slice the shard list: each <rank,worker> gets every N‑th file
        shards = self.shard_paths.copy()
        if self.shuffle:
            random.Random(self.epoch).shuffle(shards)

        # unique stride = world * num_workers
        stride = self.world * self.num_workers
        start = self.rank * self.num_workers + self.worker_id
        my_shards = shards[start::stride]

        for path in my_shards:
            mm, n_tokens = self._open_shard(path)
            yield from self._iter_one_shard(mm, n_tokens)

    def set_epoch(self, epoch: int):
        """
        Set the epoch number.

        NOTE: The `DataLoader` calls this hook at the end of each epoch.

        Args:
            epoch: The epoch number.
        """
        self._epoch = epoch


def create_dataloader(
    ds: TokenShardDataset,
    batch_size: int = DEFAULT_BATCH_SIZE,
    num_workers: int = DEFAULT_N_PROCS,
    pin_memory: bool = True,
    prefetch_factor: int = DEFAULT_PREFETCH_FACTOR,
    persistent_workers: bool = True,
    drop_last: bool = True,
):
    """
    Create a data loader for the token shard dataset.

    Args:
        ds: The dataset to create a data loader for.
        batch_size: The number of sequences to process together in each iteration.
        num_workers: The number of subprocesses for data loading. Number of subprocesses for data
            loading. This enables parallel data loading using multiple worker processes
        pin_memory: If True, the data loader will copy tensors into CUDA pinned memory. This speeds
            up GPU transfer by using page-locked memory that can't be swapped out.
        prefetch_factor: The number of batches loaded in advance by each worker. This helps maintain
            a steady flow of data to the GPU.
        persistent_workers: If True, worker processes are not killed between epochs. This avoids
            the overhead of recreating workers for each epoch
        drop_last: If True, drops the last incomplete batch if dataset size is not divisible by
            batch_size. This ensures all batches have the same size, which is often required
            for training.
    """
    # Initialize the data loader.
    # NOTE: collate_fn: Custom function to merge a list of samples into a batch.
    #   Here it takes a batch of (data, labels) tuples and stacks them into tensors.
    #   The lambda function: lambda batch: tuple(torch.stack(x) for x in zip(*batch))
    #       - zip(*batch) groups all data samples together and all label samples together.
    #       - torch.stack(x) converts each group into a single tensor.
    #       - tuple(...) returns the stacked data and labels as a tuple.
    dl = torch_data.DataLoader(
        ds,
        batch_size=batch_size,
        num_workers=num_workers,
        pin_memory=pin_memory,
        prefetch_factor=prefetch_factor,
        persistent_workers=persistent_workers,
        drop_last=drop_last,
        collate_fn=lambda batch: tuple(torch.stack(x) for x in zip(*batch)),
    )

    return dl
